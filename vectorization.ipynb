{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#파일 불러오기\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shakespeare-hamlet.txt 를 사용!\n",
    "# nltk.corpus.gutenberg.sents은 마지막 \".\" 을 기준으로 문장 불러오기\n",
    "hamlet = nltk.corpus.gutenberg.sents(\"shakespeare-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nltk.corpus.reader.util.StreamBackedCorpusView, 3106)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#타입 확인\n",
    "type(hamlet), len(hamlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', ']'], ['Actus', 'Primus', '.'], ['Scoena', 'Prima', '.'], ['Enter', 'Barnardo', 'and', 'Francisco', 'two', 'Centinels', '.'], ['Barnardo', '.'], ['Who', \"'\", 's', 'there', '?'], ['Fran', '.'], ['Nay', 'answer', 'me', ':', 'Stand', '&', 'vnfold', 'your', 'selfe'], ['Bar', '.'], ['Long', 'liue', 'the', 'King'], ['Fran', '.'], ['Barnardo', '?'], ['Bar', '.'], ['He'], ['Fran', '.'], ['You', 'come', 'most', 'carefully', 'vpon', 'your', 'houre'], ['Bar', '.'], [\"'\", 'Tis', 'now', 'strook', 'twelue', ',', 'get', 'thee', 'to', 'bed', 'Francisco'], ['Fran', '.'], ['For', 'this', 'releefe', 'much', 'thankes', ':', \"'\", 'Tis', 'bitter', 'cold', ',', 'And', 'I', 'am', 'sicke', 'at', 'heart']]\n"
     ]
    }
   ],
   "source": [
    "# 앞에서 20개 문장 출력\n",
    "print(hamlet[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['[',\n",
       "  'The',\n",
       "  'Tragedie',\n",
       "  'of',\n",
       "  'Hamlet',\n",
       "  'by',\n",
       "  'William',\n",
       "  'Shakespeare',\n",
       "  '1599',\n",
       "  ']'],\n",
       " ['Actus', 'Primus', '.'],\n",
       " ['Scoena', 'Prima', '.'],\n",
       " ['Enter', 'Barnardo', 'and', 'Francisco', 'two', 'Centinels', '.'],\n",
       " ['Barnardo', '.'],\n",
       " ['Who', \"'\", 's', 'there', '?'],\n",
       " ['Fran', '.'],\n",
       " ['Nay', 'answer', 'me', ':', 'Stand', '&', 'vnfold', 'your', 'selfe'],\n",
       " ['Bar', '.'],\n",
       " ['Long', 'liue', 'the', 'King'],\n",
       " ['Fran', '.'],\n",
       " ['Barnardo', '?'],\n",
       " ['Bar', '.'],\n",
       " ['He'],\n",
       " ['Fran', '.'],\n",
       " ['You', 'come', 'most', 'carefully', 'vpon', 'your', 'houre'],\n",
       " ['Bar', '.'],\n",
       " [\"'\",\n",
       "  'Tis',\n",
       "  'now',\n",
       "  'strook',\n",
       "  'twelue',\n",
       "  ',',\n",
       "  'get',\n",
       "  'thee',\n",
       "  'to',\n",
       "  'bed',\n",
       "  'Francisco'],\n",
       " ['Fran', '.'],\n",
       " ['For',\n",
       "  'this',\n",
       "  'releefe',\n",
       "  'much',\n",
       "  'thankes',\n",
       "  ':',\n",
       "  \"'\",\n",
       "  'Tis',\n",
       "  'bitter',\n",
       "  'cold',\n",
       "  ',',\n",
       "  'And',\n",
       "  'I',\n",
       "  'am',\n",
       "  'sicke',\n",
       "  'at',\n",
       "  'heart']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Tragedie', 'of', 'Hamlet', 'by', 'William', 'Shakespeare', '1599', 'Actus', 'Primus', 'Scoena', 'Prima', 'Enter', 'Barnardo', 'and', 'Francisco', 'two', 'Centinels', 'Who', 's', 'there', 'Fran', 'Nay', 'answer', 'me', 'Stand', 'vnfold', 'your', 'selfe', 'Bar', 'Long', 'liue', 'the', 'King', 'He', 'You', 'come', 'most', 'carefully', 'vpon', 'houre', 'Tis', 'now', 'strook', 'twelue', 'get', 'thee', 'to', 'bed', 'For', 'this', 'releefe', 'much', 'thankes', 'bitter', 'cold', 'And', 'I', 'am', 'sicke', 'at', 'heart']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 사전 만들기\n",
    "data = []\n",
    "#문장 개수만큼 반복(20번)\n",
    "for line in hamlet[:20]:\n",
    "    # 각 문장에서 단어들을 word에 할당\n",
    "    for word in line:\n",
    "        # 단어들이 data 내부에 없고 알파벳이나 숫자라면\n",
    "        if word.isalnum() and word not in data:\n",
    "            #문장사전에 추가한다\n",
    "            data.append(word)\n",
    "#문장 사전 출력\n",
    "print(data)\n",
    "#문장 사전 길이 출력\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enter', 'Barnardo', 'and', 'Francisco', 'two', 'Centinels', '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장들중 4번째 문장을 sent1 에 할당\n",
    "sent1 = hamlet[3]\n",
    "sent1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization은 데이터를 벡터화해서 연산하는 것을 일컫는다.\n",
    "# 이것의 장점은 반복문을 통해 연산 했을 때보다 속도가 훨씬 빠르다는 것이다.\n",
    "# 특히나 데이터의 값이 많아질수록 속도의 차이는 더욱 심해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#벡터화 하기\n",
    "vect1 = []\n",
    "#문장 사전 개수만큼 반복(62번)\n",
    "for word in data:\n",
    "    # 4번째 문장안에 문장 사전내부의 단어가 있다면\n",
    "    if word in sent1:\n",
    "        #1로 마킹한다\n",
    "        vect1.append(1)\n",
    "    else:\n",
    "        #없다면 0으로 마킹한다\n",
    "        vect1.append(0)\n",
    "print(vect1)\n",
    "len(vect1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위의 코드를 함수화 시키는 것\n",
    "def wordToBinary(sentence, vocabulary):\n",
    "    #문장을 출력해주는 부분을 추가했다\n",
    "    print(sentence)\n",
    "    vector = []\n",
    "    for word in vocabulary:\n",
    "        if word in sentence:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nay', 'answer', 'me', ':', 'Stand', '&', 'vnfold', 'your', 'selfe']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# 문장들중 8번째 문장을 sent2 에 할당\n",
    "sent2 = hamlet[7]\n",
    "# 벡터화 시켜줌(함수 내부에 문장 출력 있음)\n",
    "vect2 = wordToBinary(sent2, data)\n",
    "#벡터 출력\n",
    "print(vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'come', 'most', 'carefully', 'vpon', 'your', 'houre']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[\"'\", 'Tis', 'now', 'strook', 'twelue', ',', 'get', 'thee', 'to', 'bed', 'Francisco']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['For', 'this', 'releefe', 'much', 'thankes', ':', \"'\", 'Tis', 'bitter', 'cold', ',', 'And', 'I', 'am', 'sicke', 'at', 'heart']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# 위 부분들을 반복함\n",
    "#16번째 문장\n",
    "vect3 = wordToBinary(hamlet[15], data)\n",
    "print(vect3)\n",
    "#18번째 문장\n",
    "vect4 = wordToBinary(hamlet[17], data)\n",
    "print(vect4)\n",
    "#20번째 문장\n",
    "vect5 = wordToBinary(hamlet[19], data)\n",
    "print(vect5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전통적인 방식의 데이터베이스\n",
    "## 컬럼의 숫자가 제한되어 있다.\n",
    "\n",
    "# 텍스트 데이터\n",
    "## 특성들이 많다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization의 장점\n",
    "\n",
    "## 특성의 수를 줄여주는 효과\n",
    "## 언어의 구문을 이해할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization을 하는 방법\n",
    "\n",
    "# 1. Bag Of Words : Unique Words, Word Frequency\n",
    "# 2. TF-IDF : \n",
    "\n",
    "## TF = Term Frequency  \n",
    "###         문장안에서 단어들이 나오는 빈도\n",
    "###  TF = ----------------------------------\n",
    "###          문장안에서 단어들의 총 갯수\n",
    "\n",
    "## IDF = Inverse Document Frequence\n",
    "###               (문서 - document) 문장들의 총 갯수\n",
    "### IDF = -------------------------------------------------\n",
    "###         단어를 포함하는 (문서 - document) 문장들의 갯수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW(Bag-of-Words)의 개념\n",
    "## 애초에 컴퓨터는 인간이 아니기 때문에 언어를 이해할 방법이 없다고 봐도 된다. 아직까지는. 그래서 어떤 식으로든 1과 0 밖에 모르는 컴퓨터가 인간의 언어 사용 패턴을 최대한 이해하도록 여러가지 방법을 강구해야 하는데....\n",
    "## 아무튼 BoW(Bag Of Words) 는 어휘의 빈도(개수)에 대해 통계적 언어 모델을 적용해서 나타낸 것이라 이해하면 된다.\n",
    "https://hleecaster.com/nlp-bag-of-words-concept/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF(Term Frequency-Inverse Document Frequency)는 단어의 빈도와 역 문서 빈도(문서의 빈도에 특정 식을 취함)를 사용하여 DTM 내의 각 단어들마다 중요한 정도를 가중치로 주는 방법입니다.\n",
    "## DTM 의 개념 https://wikidocs.net/24559"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\git\\study\\nltk\\nltk\\lib\\site-packages (1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in d:\\git\\study\\nltk\\nltk\\lib\\site-packages (from scikit-learn) (1.21.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in d:\\git\\study\\nltk\\nltk\\lib\\site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\git\\study\\nltk\\nltk\\lib\\site-packages (from scikit-learn) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in d:\\git\\study\\nltk\\nltk\\lib\\site-packages (from scikit-learn) (1.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.2.1; however, version 21.3 is available.\n",
      "You should consider upgrading via the 'd:\\git\\study\\nltk\\nltk\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원합니다\n",
    "# 이는 앞에서 했던 Vectorization의 동작원리와 같다\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#객체 만들어 주기\n",
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enter Barnardo and Francisco two Centinels .',\n",
       " 'Nay answer me : Stand & vnfold your selfe',\n",
       " 'You come most carefully vpon your houre',\n",
       " \"' Tis now strook twelue , get thee to bed Francisco\",\n",
       " \"For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sents에 문장 5개를 join한다.\n",
    "\n",
    "sents = []\n",
    "sents.append(\" \".join(hamlet[3]))\n",
    "sents.append(\" \".join(hamlet[7]))\n",
    "sents.append(\" \".join(hamlet[15]))\n",
    "sents.append(\" \".join(hamlet[17]))\n",
    "sents.append(\" \".join(hamlet[19]))\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization하는 과정(내부는 0과 1로 마킹되어 있음)\n",
    "X = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'and', 'answer', 'at', 'barnardo', 'bed', 'bitter', 'carefully', 'centinels', 'cold', 'come', 'enter', 'for', 'francisco', 'get', 'heart', 'houre', 'me', 'most', 'much', 'nay', 'now', 'releefe', 'selfe', 'sicke', 'stand', 'strook', 'thankes', 'thee', 'this', 'tis', 'to', 'twelue', 'two', 'vnfold', 'vpon', 'you', 'your']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "# 객체 내부에 저장되어 있는 sents의 이름들을 출력\n",
    "print(vectorizer.get_feature_names())\n",
    "# 단어 개수 출력\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x38 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 42 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5*38 행렬생성\n",
    "# 문장이 5개 단어가 38개 있음을 의미\n",
    "X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "        0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#List(리스트)를 Array(배열)로 변경할 때 사용하는 메서드가 toArray()이다.\n",
    "\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Enter Barnardo and Francisco two Centinels .',\n",
       " 'Nay answer me : Stand & vnfold your selfe',\n",
       " 'You come most carefully vpon your houre',\n",
       " \"' Tis now strook twelue , get thee to bed Francisco\",\n",
       " \"For this releefe much thankes : ' Tis bitter cold , And I am sicke at heart\"]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enter barnardo and francisco two centinels  ',\n",
       " 'nay answer me   stand   vnfold your selfe',\n",
       " 'you come most carefully vpon your houre',\n",
       " '  tis now strook twelue   get thee to bed francisco',\n",
       " 'for this releefe much thankes     tis bitter cold   and i am sicke at heart']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regex를 이용하여 문장 처리(문장에서 문자가 아닌건 모두 공백으로 바꿔줌)\n",
    "import re\n",
    "#문장의 길이만큼 반복(5번)\n",
    "for i in range(len(sents)):\n",
    "    # 문장을 소문자로 변경\n",
    "    sents[i] = sents[i].lower()\n",
    "    #re.sub은 문자열을 치환하는 메소드이다.\n",
    "    sents[i] = re.sub(r\"\\W\", \" \", sents[i])\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enter', 'barnardo', 'and', 'francisco', 'two', 'centinels'],\n",
       " ['nay', 'answer', 'me', 'stand', 'vnfold', 'your', 'selfe'],\n",
       " ['you', 'come', 'most', 'carefully', 'vpon', 'your', 'houre'],\n",
       " ['tis', 'now', 'strook', 'twelue', 'get', 'thee', 'to', 'bed', 'francisco'],\n",
       " ['for',\n",
       "  'this',\n",
       "  'releefe',\n",
       "  'much',\n",
       "  'thankes',\n",
       "  'tis',\n",
       "  'bitter',\n",
       "  'cold',\n",
       "  'and',\n",
       "  'i',\n",
       "  'am',\n",
       "  'sicke',\n",
       "  'at',\n",
       "  'heart']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각각의 문장을 토큰화 해서 sent_tokens에 할당\n",
    "sent_tokens = []\n",
    "for sent in sents:\n",
    "    # word_tokenize(리스트) 리스트 값을 집어넣어 줘야 한다.\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    sent_tokens.append(tokens)\n",
    "\n",
    "sent_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enter': 1,\n",
       " 'barnardo': 1,\n",
       " 'and': 2,\n",
       " 'francisco': 2,\n",
       " 'two': 1,\n",
       " 'centinels': 1,\n",
       " 'nay': 1,\n",
       " 'answer': 1,\n",
       " 'me': 1,\n",
       " 'stand': 1,\n",
       " 'vnfold': 1,\n",
       " 'your': 2,\n",
       " 'selfe': 1,\n",
       " 'you': 1,\n",
       " 'come': 1,\n",
       " 'most': 1,\n",
       " 'carefully': 1,\n",
       " 'vpon': 1,\n",
       " 'houre': 1,\n",
       " 'tis': 2,\n",
       " 'now': 1,\n",
       " 'strook': 1,\n",
       " 'twelue': 1,\n",
       " 'get': 1,\n",
       " 'thee': 1,\n",
       " 'to': 1,\n",
       " 'bed': 1,\n",
       " 'for': 1,\n",
       " 'this': 1,\n",
       " 'releefe': 1,\n",
       " 'much': 1,\n",
       " 'thankes': 1,\n",
       " 'bitter': 1,\n",
       " 'cold': 1,\n",
       " 'i': 1,\n",
       " 'am': 1,\n",
       " 'sicke': 1,\n",
       " 'at': 1,\n",
       " 'heart': 1}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어 빈도수 체크\n",
    "# 즉 Bag-of-Words를 하는 과정이다.\n",
    "\n",
    "# 단어의 빈도수를 저장할 wordfreq 딕셔너리 선언\n",
    "wordfreq = {}\n",
    "# 문장들을 토큰화한 sent_tokens의 개수만큼 반복(5번)\n",
    "for sentence in sent_tokens:\n",
    "    # 각 문장 내부의 각 단어들을\n",
    "    for word in sentence:\n",
    "        # wordfreq.keys()와 비교하여 존재하지 않는다면\n",
    "        if word not in wordfreq.keys():\n",
    "            # 키와 벨류를 생성\n",
    "            wordfreq[word] = 1\n",
    "        else:\n",
    "            # wordfreq.keys()와 비교하여 존재 한다면\n",
    "            # 빈도수를 늘려준다\n",
    "            wordfreq[word] += 1\n",
    "\n",
    "wordfreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lamda는 함수를 한줄로 만들어 주는 신기한 녀석\n",
    "## lambda 인자 : 표현식 이 기본 형태\n",
    "# ex)함수가 이렇게 생겼다면\n",
    "--def hap(x, y):\n",
    "\n",
    "-   return x + y\n",
    "\n",
    "--hap(10, 20)\n",
    "-    30\n",
    "# ex)lamda는 이렇게 생겼다.\n",
    "--(lambda x,y: x + y)(10, 20)\n",
    "-  30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(정렬할 데이터, key 파라미터, reverse 파라미터)\n",
    "# 이 부분의 람다식이 어떻게 동작하는지는 이해하지 못했음 아는분 있으면 알려주길 바람\n",
    "# 결국 단어 빈도수를 정렬하라는 것\n",
    "sort_wordfreq = sorted(wordfreq.items(), key = lambda kv:(kv[1], kv[0]), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('your', 2),\n",
       " ('tis', 2),\n",
       " ('francisco', 2),\n",
       " ('and', 2),\n",
       " ('you', 1),\n",
       " ('vpon', 1),\n",
       " ('vnfold', 1),\n",
       " ('two', 1),\n",
       " ('twelue', 1),\n",
       " ('to', 1),\n",
       " ('this', 1),\n",
       " ('thee', 1),\n",
       " ('thankes', 1),\n",
       " ('strook', 1),\n",
       " ('stand', 1),\n",
       " ('sicke', 1),\n",
       " ('selfe', 1),\n",
       " ('releefe', 1),\n",
       " ('now', 1),\n",
       " ('nay', 1),\n",
       " ('much', 1),\n",
       " ('most', 1),\n",
       " ('me', 1),\n",
       " ('i', 1),\n",
       " ('houre', 1),\n",
       " ('heart', 1),\n",
       " ('get', 1),\n",
       " ('for', 1),\n",
       " ('enter', 1),\n",
       " ('come', 1),\n",
       " ('cold', 1),\n",
       " ('centinels', 1),\n",
       " ('carefully', 1),\n",
       " ('bitter', 1),\n",
       " ('bed', 1),\n",
       " ('barnardo', 1),\n",
       " ('at', 1),\n",
       " ('answer', 1),\n",
       " ('am', 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sort_wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['your', 'tis', 'francisco', 'and', 'you']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수 데이터가 너무 많으니 5개만 잘라서 가장 많은 빈도를 5개 출력\n",
    "most_freq = []\n",
    "for word in sort_wordfreq[:5]:\n",
    "    most_freq.append(word[0])\n",
    "most_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 단어 | 총 문장 | 빈도수 | 총 문장 / 빈도수 |\n",
    "| word | total sentences | sentence has word | total sents / sent has word |\n",
    "| --- | --- | --- | --- |\n",
    "| and | 5 | 2 | 2.5 |\n",
    "| francisco | 5 | 2 | 2.5 |\n",
    "| tis | 5 | 2 | 2.5 |\n",
    "| you | 5 | 1 | 5 |\n",
    "| your | 5 | 2 |  2.5 |\n",
    "\n",
    "즉 idf 구하는 것임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'your': 0.5108256237659907,\n",
       " 'tis': 0.5108256237659907,\n",
       " 'francisco': 0.5108256237659907,\n",
       " 'and': 0.5108256237659907,\n",
       " 'you': 0.9162907318741551}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 부분은 이해가 잘 가지 않는다.\n",
    "# 내부의 로직이 어렵다 나중에 다시 리마인드 하자\n",
    "# 대충 기능만 보면 most_freq에서 token 가져와서 sent리스트와 비교하여 있으면 1씩 추가시켜주는 것임\n",
    "import numpy as np \n",
    "\n",
    "word_idf_values = {}\n",
    "for token in most_freq:\n",
    "    sent_has_word = 0\n",
    "    for sent in sent_tokens:\n",
    "        if token in sent:\n",
    "            sent_has_word += 1\n",
    "        # 아래부분이 이해 안감\n",
    "        word_idf_values[token] = np.log(len(sents) / (1 + sent_has_word))\n",
    "\n",
    "word_idf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'your': [0.0, 0.14285714285714285, 0.14285714285714285, 0.0, 0.0],\n",
       " 'tis': [0.0, 0.0, 0.0, 0.1111111111111111, 0.07142857142857142],\n",
       " 'francisco': [0.16666666666666666, 0.0, 0.0, 0.1111111111111111, 0.0],\n",
       " 'and': [0.16666666666666666, 0.0, 0.0, 0.0, 0.07142857142857142],\n",
       " 'you': [0.0, 0.0, 0.14285714285714285, 0.0, 0.0]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기는 tf 구하는 로직이다.\n",
    "# 여기도 아직 이해가 부족하다.\n",
    "word_tf_values = {}\n",
    "# 5번 반복\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for sent in sent_tokens:\n",
    "        doc_freq = 0\n",
    "        for word in sent:\n",
    "            if token == word:\n",
    "                # 문장 내부의 단어와 빈도수 단어가 같다면 doc_freq에 1을 추가함\n",
    "                doc_freq += 1\n",
    "        word_tf = doc_freq / len(sent)\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    # 딕셔너리 형태로 token마다 중요도를 만들어중\n",
    "    word_tf_values[token] = sent_tf_vector\n",
    "\n",
    "word_tf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.07297508910942724, 0.07297508910942724, 0.0, 0.0],\n",
       " [0.0, 0.0, 0.0, 0.05675840264066563, 0.03648754455471362],\n",
       " [0.08513760396099845, 0.0, 0.0, 0.05675840264066563, 0.0],\n",
       " [0.08513760396099845, 0.0, 0.0, 0.0, 0.03648754455471362],\n",
       " [0.0, 0.0, 0.13089867598202215, 0.0, 0.0]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tf-idf 고려한 중요도?\n",
    "tfidf_values = []\n",
    "\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)\n",
    "\n",
    "tfidf_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.07297509, 0.07297509, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.0567584 , 0.03648754],\n",
       "       [0.0851376 , 0.        , 0.        , 0.0567584 , 0.        ],\n",
       "       [0.0851376 , 0.        , 0.        , 0.        , 0.03648754],\n",
       "       [0.        , 0.        , 0.13089868, 0.        , 0.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model = np.asarray(tfidf_values)\n",
    "tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idf_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.0851376 , 0.0851376 , 0.        ],\n",
       "       [0.07297509, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.07297509, 0.        , 0.        , 0.        , 0.13089868],\n",
       "       [0.        , 0.0567584 , 0.0567584 , 0.        , 0.        ],\n",
       "       [0.        , 0.03648754, 0.        , 0.03648754, 0.        ]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model = np.transpose(tf_idf_model)\n",
    "tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위에서 했던 고생을 한방에 해주는 고마운 사이킷 런\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체 만들고\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enter barnardo and francisco two centinels  ',\n",
       " 'nay answer me   stand   vnfold your selfe',\n",
       " 'you come most carefully vpon your houre',\n",
       " '  tis now strook twelue   get thee to bed francisco',\n",
       " 'for this releefe much thankes     tis bitter cold   and i am sicke at heart']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit_transform 메서드에 문장을 집어넣으면 벡터화 해줌\n",
    "X = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'and', 'answer', 'at', 'barnardo', 'bed', 'bitter', 'carefully', 'centinels', 'cold', 'come', 'enter', 'for', 'francisco', 'get', 'heart', 'houre', 'me', 'most', 'much', 'nay', 'now', 'releefe', 'selfe', 'sicke', 'stand', 'strook', 'thankes', 'thee', 'this', 'tis', 'to', 'twelue', 'two', 'vnfold', 'vpon', 'you', 'your']\n",
      "38\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(len(vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.35038823, 0.        , 0.        , 0.43429718,\n",
       "        0.        , 0.        , 0.        , 0.43429718, 0.        ,\n",
       "        0.        , 0.43429718, 0.        , 0.35038823, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.43429718, 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.38775666, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38775666, 0.        , 0.        ,\n",
       "        0.38775666, 0.        , 0.        , 0.38775666, 0.        ,\n",
       "        0.38775666, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.38775666,\n",
       "        0.        , 0.        , 0.31283963],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.38775666, 0.        , 0.        ,\n",
       "        0.38775666, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.38775666, 0.        , 0.38775666, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.38775666, 0.38775666, 0.31283963],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34706676, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.28001128, 0.34706676,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34706676, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34706676, 0.        , 0.34706676, 0.        ,\n",
       "        0.28001128, 0.34706676, 0.34706676, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.28511174, 0.23002636, 0.        , 0.28511174, 0.        ,\n",
       "        0.        , 0.28511174, 0.        , 0.        , 0.28511174,\n",
       "        0.        , 0.        , 0.28511174, 0.        , 0.        ,\n",
       "        0.28511174, 0.        , 0.        , 0.        , 0.28511174,\n",
       "        0.        , 0.        , 0.28511174, 0.        , 0.28511174,\n",
       "        0.        , 0.        , 0.28511174, 0.        , 0.28511174,\n",
       "        0.23002636, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 벡터화 한거 출력해보면\n",
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#끝"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f2a156638663e52e3310a5b94cba9f1aaa2c6363cff10141cd0a597f3d9860"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6rc1 64-bit ('nltk': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
