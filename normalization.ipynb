{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = '''Though Joey was tired of running the whole day, he still ran as fast \n",
    "as he could to escape the chasing beast. As he was a marathon runner, he was \n",
    "always ready for a long run but refraining from food for past 2 days had taken \n",
    "a toll on his muscles.'''\n",
    "\n",
    "text2 = '''All the players looked very tired on the very second day of the game. \n",
    "Although the Test Cricket is a very tiring format for players but this time the \n",
    "cause of such tiredness was the hot, dry climate of the venue and not the format\n",
    "itself as a physical conditioning team prepares players to play in the Test \n",
    "format without getting tired.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizaion을 해보자!\n",
    "from nltk import word_tokenize\n",
    "tokens1 = word_tokenize(text1)\n",
    "tokens2 = word_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'Joey', 'was', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'was', 'a', 'marathon', 'runner', ',', 'he', 'was', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'days', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'the', 'players', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'players', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'was', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physical', 'conditioning', 'team', 'prepares', 'players', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Text Processing(고급 텍스트 처리)\n",
    "# * stemming(어근 찾기)\n",
    "# * Lemmatisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "영어 자연어처리를 위한 전처리 과정에서는 동사의 어간을 추출하기 위한 'stemming' 과정이 포함되기도 합니다. \n",
    "\n",
    "영어는 굴절어로 분류되며, 그 특징을 동사에서 발견할 수 있습니다. 물론 명사나 형용사에서 나타나는 접사들의 결합 여기에 포함될 수 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "출처: https://skyjwoo.tistory.com/entry/Porter-Stemmer포터-스테머를-만들어보자 [jeongstudy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming : 어근\n",
    "# Stemming API\n",
    "# - Porter\n",
    "# - Lancaster\n",
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'Joey', 'was', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'was', 'a', 'marathon', 'runner', ',', 'he', 'was', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'days', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['though', 'joey', 'wa', 'tire', 'of', 'run', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escap', 'the', 'chase', 'beast', '.', 'as', 'he', 'wa', 'a', 'marathon', 'runner', ',', 'he', 'wa', 'alway', 'readi', 'for', 'a', 'long', 'run', 'but', 'refrain', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'taken', 'a', 'toll', 'on', 'hi', 'muscl', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(token) for token in tokens1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'the', 'players', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'players', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'was', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physical', 'conditioning', 'team', 'prepares', 'players', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'the', 'player', 'look', 'veri', 'tire', 'on', 'the', 'veri', 'second', 'day', 'of', 'the', 'game', '.', 'although', 'the', 'test', 'cricket', 'is', 'a', 'veri', 'tire', 'format', 'for', 'player', 'but', 'thi', 'time', 'the', 'caus', 'of', 'such', 'tired', 'wa', 'the', 'hot', ',', 'dri', 'climat', 'of', 'the', 'venu', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physic', 'condit', 'team', 'prepar', 'player', 'to', 'play', 'in', 'the', 'test', 'format', 'without', 'get', 'tire', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(token) for token in tokens2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import LancasterStemmer\n",
    "ls = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'Joey', 'was', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'was', 'a', 'marathon', 'runner', ',', 'he', 'was', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'days', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['though', 'joey', 'was', 'tir', 'of', 'run', 'the', 'whol', 'day', ',', 'he', 'stil', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escap', 'the', 'chas', 'beast', '.', 'as', 'he', 'was', 'a', 'marathon', 'run', ',', 'he', 'was', 'alway', 'ready', 'for', 'a', 'long', 'run', 'but', 'refrain', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'tak', 'a', 'tol', 'on', 'his', 'musc', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ls.stem(token) for token in tokens1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'the', 'players', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'players', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'was', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physical', 'conditioning', 'team', 'prepares', 'players', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['al', 'the', 'play', 'look', 'very', 'tir', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'gam', '.', 'although', 'the', 'test', 'cricket', 'is', 'a', 'very', 'tir', 'form', 'for', 'play', 'but', 'thi', 'tim', 'the', 'caus', 'of', 'such', 'tir', 'was', 'the', 'hot', ',', 'dry', 'clim', 'of', 'the', 'venu', 'and', 'not', 'the', 'form', 'itself', 'as', 'a', 'phys', 'condit', 'team', 'prep', 'play', 'to', 'play', 'in', 'the', 'test', 'form', 'without', 'get', 'tir', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ls.stem(token) for token in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "## * wordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "WordNet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'Joey', 'was', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'as', 'fast', 'as', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'was', 'a', 'marathon', 'runner', ',', 'he', 'was', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'days', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscles', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Though', 'Joey', 'wa', 'tired', 'of', 'running', 'the', 'whole', 'day', ',', 'he', 'still', 'ran', 'a', 'fast', 'a', 'he', 'could', 'to', 'escape', 'the', 'chasing', 'beast', '.', 'As', 'he', 'wa', 'a', 'marathon', 'runner', ',', 'he', 'wa', 'always', 'ready', 'for', 'a', 'long', 'run', 'but', 'refraining', 'from', 'food', 'for', 'past', '2', 'day', 'had', 'taken', 'a', 'toll', 'on', 'his', 'muscle', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ WordNet.lemmatize(token) for token in tokens1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'the', 'players', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'players', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'was', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'as', 'a', 'physical', 'conditioning', 'team', 'prepares', 'players', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'the', 'player', 'looked', 'very', 'tired', 'on', 'the', 'very', 'second', 'day', 'of', 'the', 'game', '.', 'Although', 'the', 'Test', 'Cricket', 'is', 'a', 'very', 'tiring', 'format', 'for', 'player', 'but', 'this', 'time', 'the', 'cause', 'of', 'such', 'tiredness', 'wa', 'the', 'hot', ',', 'dry', 'climate', 'of', 'the', 'venue', 'and', 'not', 'the', 'format', 'itself', 'a', 'a', 'physical', 'conditioning', 'team', 'prepares', 'player', 'to', 'play', 'in', 'the', 'Test', 'format', 'without', 'getting', 'tired', '.']\n"
     ]
    }
   ],
   "source": [
    "print([ WordNet.lemmatize(token) for token in tokens2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization: Stemming과의 차이\n",
    "## Lemmatization이란 문장 속에서 다양한 형태로 활용된(inflected) 단어의 표제어(lemma)를 찾는 일을 뜻한다. 여기서 말하는 표제어란 사전에서 단어의 뜻을 찾을 때 쓰는 기본형이라고 생각하면 된다. 예를 들어, ‘아름다운’이 Lemmatization을 거치면 ‘아름답다’가 된다. Stemming과 비슷해 보이지만 중요한 차이가 있다. Stemming은 단어 그 자체만을 고려하지만 Lemmatization은 그 단어가 문장 속에서 어떤 품사(Part-of-speech)로 쓰였는지까지 판단한다.\n",
    "한국어에서는 적절한 예가 떠오르지 않아서 영어를 예로 들면, () ‘flies’가 주어졌을 때 Stemming은 단순히 이 단어의 어근을 내놓는데 비해 Lemmatization은 문장 속에서 ‘files’가 동사 ‘날다’ 와 명사 ‘파리’ 중 어떤 뜻으로 쓰였는지까지 결정할 수 있어야 한다. 그렇기 때문에 Lemmatization을 수행하려면 문장 구조 분석 같은 언어적 이해가 필요하고 따라서 Stemming에 비해 복잡한 처리 과정을 거쳐야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part-Of=speech(품사) => POS tagging\n",
    "# fly : 동사 => 날다/ 명사 => 파리\n",
    "#Part-Of-Speech tagging(POS tagging)은 문장 내 단어들의 품사를 식별하여 태그를 붙여주는 것을 말한다.\n",
    "# 투플(tuple)의 형태로 출력되며 (단어, 태그)로 출력된다. 여기서 태그는 품사(POS) 태그다."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f2a156638663e52e3310a5b94cba9f1aaa2c6363cff10141cd0a597f3d9860"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6rc1 64-bit ('nltk': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
