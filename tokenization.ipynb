{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'book'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection book\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Although Gainesville  isn’t really a ‘big city’, it’s certainly not a \n",
    "          tiny village either. It was recently voted the 9th best college town \n",
    "          in America, perhaps because of its extremely student-friendly nature. \n",
    "          There’s enough  to do, and it’s pretty self-sufficient, so to speak. \n",
    "          Of course, if you need a breather, you could always make the hour long \n",
    "          journeys to Jacksonville or Orlando, and Miami is never too far.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Although Gainesville  isn’t really a ‘big city’, it’s certainly not a \\n          tiny village either. It was recently voted the 9th best college town \\n          in America, perhaps because of its extremely student-friendly nature. \\n          There’s enough  to do, and it’s pretty self-sufficient, so to speak. \\n          Of course, if you need a breather, you could always make the hour long \\n          journeys to Jacksonville or Orlando, and Miami is never too far.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'Gainesville', 'isn’t', 'really', 'a', '‘big', 'city’,', 'it’s', 'certainly', 'not', 'a', 'tiny', 'village', 'either.', 'It', 'was', 'recently', 'voted', 'the', '9th', 'best', 'college', 'town', 'in', 'America,', 'perhaps', 'because', 'of', 'its', 'extremely', 'student-friendly', 'nature.', 'There’s', 'enough', 'to', 'do,', 'and', 'it’s', 'pretty', 'self-sufficient,', 'so', 'to', 'speak.', 'Of', 'course,', 'if', 'you', 'need', 'a', 'breather,', 'you', 'could', 'always', 'make', 'the', 'hour', 'long', 'journeys', 'to', 'Jacksonville', 'or', 'Orlando,', 'and', 'Miami', 'is', 'never', 'too', 'far.']\n"
     ]
    }
   ],
   "source": [
    "#단어 분리\n",
    "#파이썬 string 클래스의 split() 메서드 사용하는 방법 => 무조건 빈 칸을 기준으로 자름\n",
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규식 regular expression 사용\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'Gainesville', '', 'isn’t', 'really', 'a', '‘big', 'city’,', 'it’s', 'certainly', 'not', 'a', '\\n', '', '', '', '', '', '', '', '', '', 'tiny', 'village', 'either.', 'It', 'was', 'recently', 'voted', 'the', '9th', 'best', 'college', 'town', '\\n', '', '', '', '', '', '', '', '', '', 'in', 'America,', 'perhaps', 'because', 'of', 'its', 'extremely', 'student-friendly', 'nature.', '\\n', '', '', '', '', '', '', '', '', '', 'There’s', 'enough', '', 'to', 'do,', 'and', 'it’s', 'pretty', 'self-sufficient,', 'so', 'to', 'speak.', '\\n', '', '', '', '', '', '', '', '', '', 'Of', 'course,', 'if', 'you', 'need', 'a', 'breather,', 'you', 'could', 'always', 'make', 'the', 'hour', 'long', '\\n', '', '', '', '', '', '', '', '', '', 'journeys', 'to', 'Jacksonville', 'or', 'Orlando,', 'and', 'Miami', 'is', 'never', 'too', 'far.']\n"
     ]
    }
   ],
   "source": [
    "print(re.split(r\" \",text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Although', 'Gainesville', 'isn’t', 'really', 'a', '‘big', 'city’,', 'it’s', 'certainly', 'not', 'a', 'tiny', 'village', 'either.', 'It', 'was', 'recently', 'voted', 'the', '9th', 'best', 'college', 'town', 'in', 'America,', 'perhaps', 'because', 'of', 'its', 'extremely', 'student-friendly', 'nature.', 'There’s', 'enough', 'to', 'do,', 'and', 'it’s', 'pretty', 'self-sufficient,', 'so', 'to', 'speak.', 'Of', 'course,', 'if', 'you', 'need', 'a', 'breather,', 'you', 'could', 'always', 'make', 'the', 'hour', 'long', 'journeys', 'to', 'Jacksonville', 'or', 'Orlando,', 'and', 'Miami', 'is', 'never', 'too', 'far.']\n"
     ]
    }
   ],
   "source": [
    "#개행문자 지우기 + 빈칸 지우기 \n",
    "#regex사용하여 분리\n",
    "print(re.split(r\"[ \\t\\n]+\",text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''Apple's world-wide annual revenue totaled $274.5 billion for the 2020 \n",
    "          fiscal year. Apple is the world's largest technology company by \n",
    "          revenue(income) and one of the world's most valuable companies.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Apple's\", 'world-wide', 'annual', 'revenue', 'totaled', '$274.5', 'billion', 'for', 'the', '2020', 'fiscal', 'year.', 'Apple', 'is', 'the', \"world's\", 'largest', 'technology', 'company', 'by', 'revenue(income)', 'and', 'one', 'of', 'the', \"world's\", 'most', 'valuable', 'companies.']\n"
     ]
    }
   ],
   "source": [
    "print(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex규칙 만들기\n",
    "#nltk regular function을 사용하는 방법\n",
    "rules = r\"\\w+(?:-\\w+)*|\\%?\\d+(?:\\.\\d+)?%?|[][.,;\\\"'?():-_`]\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', \"'\", 's', 'world-wide', 'annual', 'revenue', 'totaled', '274', '.', '5', 'billion', 'for', 'the', '2020', 'fiscal', 'year', '.', 'Apple', 'is', 'the', 'world', \"'\", 's', 'largest', 'technology', 'company', 'by', 'revenue', '(', 'income', ')', 'and', 'one', 'of', 'the', 'world', \"'\", 's', 'most', 'valuable', 'companies', '.']\n"
     ]
    }
   ],
   "source": [
    "#regex인데 rule이 뒤로간다는 점이 특이하다\n",
    "print(nltk.regexp_tokenize(text,rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', \"'s\", 'world-wide', 'annual', 'revenue', 'totaled', '$', '274.5', 'billion', 'for', 'the', '2020', 'fiscal', 'year', '.', 'Apple', 'is', 'the', 'world', \"'s\", 'largest', 'technology', 'company', 'by', 'revenue', '(', 'income', ')', 'and', 'one', 'of', 'the', 'world', \"'s\", 'most', 'valuable', 'companies', '.']\n"
     ]
    }
   ],
   "source": [
    "#nltk가 가진 내장 tokenier를 사용\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이전까지는 문장이 1개든 여러개든 무조건 단어 위주로 tokenization을 해주었다.?\n",
    "#특정 문단을 문장이라는 기준으로 tokenization을 하고 싶다.(nltk.sent_tokenize)\n",
    "#구두점을 기준으로 나눠준다\n",
    "para = '''We all are going through a pandemic and because of the spread of this coronavirus, more than a million people have already lost their lives across the world. Economy is affected because of lockdowns. With Unlock now in effect, it is our responsibility to take precautions so that another cycle does not start. Everyone is accepting new habits like wearing a mask, using sanitizer etc. as a new normal. In this huge society, you do find exceptions - people not wearing masks may come close enough to you.'''\n",
    "sentences = nltk.sent_tokenize(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 6)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences), len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We all are going through a pandemic and because of the spread of this coronavirus, more than a million people have already lost their lives across the world.',\n",
       " 'Economy is affected because of lockdowns.',\n",
       " 'With Unlock now in effect, it is our responsibility to take precautions so that another cycle does not start.',\n",
       " 'Everyone is accepting new habits like wearing a mask, using sanitizer etc.',\n",
       " 'as a new normal.',\n",
       " 'In this huge society, you do find exceptions - people not wearing masks may come close enough to you.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리에서 나눠준 토큰이 맘에 안들면 직접 regex를 만들어서 사용하면 된다.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c0f2a156638663e52e3310a5b94cba9f1aaa2c6363cff10141cd0a597f3d9860"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6rc1 64-bit ('nltk': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
